import torch
import time
import random
import numpy as np
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

# è®¾ç½®æ¨¡å‹è·¯å¾„ (è¿™é‡Œä½¿ç”¨ HuggingFace å®˜æ–¹ IDï¼Œä½ å¯ä»¥æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„)
# æ³¨æ„ï¼šLlama 3 æ²¡æœ‰ 3B ç‰ˆæœ¬ï¼Œè¿™é‡Œå‡è®¾ä½ æŒ‡çš„æ˜¯ Llama 3.2 3B å’Œ 1B
TARGET_MODEL_ID = "meta-llama/Llama-3.2-3B-Instruct"
DRAFT_MODEL_ID = "meta-llama/Llama-3.2-1B-Instruct"

device = "cuda" if torch.cuda.is_available() else "cpu"

def load_models():
    print(f"æ­£åœ¨åŠ è½½ç›®æ ‡æ¨¡å‹: {TARGET_MODEL_ID} ...")
    tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL_ID)
    
    # åŠ è½½å¤§æ¨¡å‹ (Target Model)
    target_model = AutoModelForCausalLM.from_pretrained(
        TARGET_MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    print(f"æ­£åœ¨åŠ è½½è‰ç¨¿æ¨¡å‹: {DRAFT_MODEL_ID} ...")
    # åŠ è½½å°æ¨¡å‹ (Draft/Assistant Model)
    draft_model = AutoModelForCausalLM.from_pretrained(
        DRAFT_MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    
    return tokenizer, target_model, draft_model

def load_math_questions(num_samples=10):
    """åŠ è½½ MATH-500 æ•°æ®é›†å¹¶éšæœºé‡‡æ ·"""
    print(f"\næ­£åœ¨åŠ è½½ HuggingFaceH4/MATH-500 æ•°æ®é›†...")
    try:
        # åŠ è½½æµ‹è¯•é›†
        dataset = load_dataset("HuggingFaceH4/MATH-500", split="test")
        problems = dataset["problem"]
        
        # éšæœºé‡‡æ ·
        if len(problems) < num_samples:
            sampled_problems = problems
        else:
            sampled_problems = random.sample(problems, num_samples)
            
        print(f"âœ… æˆåŠŸé‡‡æ · {len(sampled_problems)} ä¸ªæ•°å­¦é—®é¢˜ã€‚")
        return sampled_problems
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        print("âš ï¸ å°†ä½¿ç”¨å¤‡ç”¨ç”Ÿæˆçš„ç®€å•æ•°å­¦é—®é¢˜è¿›è¡Œæµ‹è¯•ã€‚")
        # å¤‡ç”¨é—®é¢˜
        return [f"Solve the following math problem: Calculate the integral of x^{i} + {i}x from 0 to 10." for i in range(1, num_samples + 1)]

def run_inference(name, model, tokenizer, inputs, assistant_model=None):
    # print(f"è¿è¡Œ: {name}") 
    # å‡å°‘åˆ·å±ï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯
    
    start_time = time.time()
    
    # æ ¸å¿ƒä»£ç ï¼šå¦‚æœæœ‰ assistant_modelï¼Œtransformers ä¼šè‡ªåŠ¨å¯ç”¨ Speculative Decoding
    output = model.generate(
        **inputs,
        assistant_model=assistant_model, # å…³é”®å‚æ•°
        max_new_tokens=200,              # ç”Ÿæˆé•¿åº¦
        do_sample=True,                  # é‡‡æ ·æ¨¡å¼
        temperature=0.6,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # è®¡ç®—é€Ÿåº¦
    generated_tokens = output.shape[1] - inputs["input_ids"].shape[1]
    speed = generated_tokens / total_time
    
    # decoded_text = tokenizer.decode(output[0], skip_special_tokens=True)
    # print(f"  -> ç”Ÿæˆ {generated_tokens} tokens, è€—æ—¶ {total_time:.2f}s, é€Ÿåº¦ {speed:.2f} t/s")
    
    return speed, generated_tokens

def main():
    tokenizer, target_model, draft_model = load_models()
    
    # é‡‡æ · 10 ä¸ªé—®é¢˜
    N_SAMPLES = 10
    questions = load_math_questions(num_samples=N_SAMPLES)
    
    std_speeds = []
    spec_speeds = []
    
    print(f"\nğŸš€ å¼€å§‹ {N_SAMPLES} è½® Math500 æµ‹è¯•å¯¹æ¯”...\n")
    print(f"{'Sample':<8} | {'Standard (t/s)':<15} | {'Speculative (t/s)':<18} | {'Speedup':<10}")
    print("-" * 60)

    for i, prompt in enumerate(questions):
        # æ„é€  promptï¼Œè¿™é‡Œç®€å•åŠ ä¸Š Instruct æ ¼å¼ï¼ˆå¦‚æœæ¨¡å‹éœ€è¦ chat template æ›´å¥½ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        # Llama 3 é€šå¸¸å»ºè®®ç”¨ chat templateï¼Œè¿™é‡Œç›´æ¥ç”± tokenizer å¤„ç† inputs
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # 1. æ ‡å‡†æ¨ç† (Standard Inference)
        speed_std, _ = run_inference(
            "Standard", target_model, tokenizer, inputs, assistant_model=None
        )
        std_speeds.append(speed_std)
        
        # 2. æŠ•æœºé‡‡æ · (Speculative Decoding)
        speed_spec, _ = run_inference(
            "Speculative", target_model, tokenizer, inputs, assistant_model=draft_model
        )
        spec_speeds.append(speed_spec)
        
        # è®¡ç®—å•æ¬¡åŠ é€Ÿæ¯”
        ratio = speed_spec / speed_std
        print(f"{i+1:<8} | {speed_std:<15.2f} | {speed_spec:<18.2f} | {ratio:<10.2f}x")

    # ç»Ÿè®¡ç»“æœ
    avg_std = np.mean(std_speeds)
    avg_spec = np.mean(spec_speeds)
    avg_ratio = avg_spec / avg_std # æ€»ä½“åŠ é€Ÿæ¯” (ä¹Ÿå¯ä»¥è®¡ç®— ratio çš„å¹³å‡å€¼ np.mean(spec_speeds / std_speeds))
    mean_ratio = np.mean(np.array(spec_speeds) / np.array(std_speeds))

    print("\n" + "=" * 60)
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡ç»“æœ")
    print("=" * 60)
    print(f"å¹³å‡æ ‡å‡†é€Ÿåº¦: {avg_std:.2f} tokens/sec")
    print(f"å¹³å‡æŠ•æœºé€Ÿåº¦: {avg_spec:.2f} tokens/sec")
    print(f"å¹³å‡åŠ é€Ÿæ¯”:   {mean_ratio:.2f}x")
    
    if mean_ratio > 1.0:
        print("\nâœ… æŠ•æœºé‡‡æ ·åœ¨æ•°å­¦é—®é¢˜ä¸Šå¸¦æ¥äº†åŠ é€Ÿï¼")
    else:
        print("\nâš ï¸ æŠ•æœºé‡‡æ ·æœªå¸¦æ¥åŠ é€Ÿã€‚å¯èƒ½åŸå› ï¼š")
        print("1. æ•°å­¦æ¨ç†é€»è¾‘æ€§å¼ºï¼Œå°æ¨¡å‹éš¾ä»¥å‡†ç¡®é¢„æµ‹å¤§æ¨¡å‹çš„å¤æ‚æ¨ç†æ­¥éª¤ï¼ˆæ¥å—ç‡ä½ï¼‰ã€‚")
        print("2. æ˜¾å¡è´Ÿè½½æˆ–æ¨¡å‹å¤§å°å·®å¼‚ä¸è¶³ä»¥æŠµæ¶ˆéªŒè¯å¼€é”€ã€‚")

if __name__ == "__main__":
    main()