#!/usr/bin/env python
"""Token 统计详解：是否包含重复的 prefix"""

print("=" * 70)
print("Token 统计详解：active beam tokens 是否包含重复的 prefix")
print("=" * 70)

print("\n【答案】：NO - 不包含重复的 prefix（prompt）tokens")
print()

print("=" * 70)
print("详细说明")
print("=" * 70)

print("\n1. vLLM 的输出结构：")
print("-" * 70)
print("   vLLM 返回的 CompletionOutput 对象：")
print("   - output.outputs[0].text: 生成的文本")
print("   - output.outputs[0].token_ids: 生成的 token IDs")
print()
print("   关键点：token_ids 只包含生成的 tokens，不包含 prompt tokens")
print()

print("2. 在 beam search 中的统计：")
print("-" * 70)
print("   每次迭代：")
print("   - 输入：prompt + current_text（已生成的部分）")
print("   - 输出：新生成的 tokens")
print("   - 统计：beam.completion_tokens += len(output.outputs[0].token_ids)")
print()
print("   所以 beam.completion_tokens 只累计新生成的 tokens，")
print("   不会重复统计 prompt 或之前已生成的部分！")
print()

print("3. 具体例子：")
print("-" * 70)
print("   假设：")
print("   - Prompt: 'Solve: 2+2=' (假设 5 tokens)")
print("   - 16 个 beams 并行生成")
print()
print("   第一次迭代：")
print("   - 输入给 vLLM: 'Solve: 2+2=' (每个 beam)")
print("   - 输出: '4' (假设 1 token)")
print("   - 统计: 每个 beam 的 completion_tokens = 1")
print("   - 16 个 beams 总计: 16 tokens")
print()
print("   第二次迭代：")
print("   - 输入给 vLLM: 'Solve: 2+2=4' (每个 beam)")
print("   - 输出: ' because' (假设 2 tokens)")
print("   - 统计: 每个 beam 的 completion_tokens = 1 + 2 = 3")
print("   - 16 个 beams 总计: 48 tokens")
print()
print("   注意：")
print("   - Prompt 的 5 tokens 没有被统计！")
print("   - 第二次迭代时，'4' 已经在 current_text 中，")
print("     vLLM 只返回新生成的 ' because'，不会再统计 '4'")
print()

print("4. total_active_beam_tokens 的含义：")
print("-" * 70)
print("   total_active_beam_tokens = sum(beam.completion_tokens for beam in active_beams)")
print()
print("   这个值表示：")
print("   ✓ 所有 active beams 生成的 completion tokens 的总和")
print("   ✓ 不包含 prompt tokens")
print("   ✓ 不重复计算已生成的 tokens")
print("   ✓ 每个 beam 独立统计，所以 16 个 beams 各自的 tokens 会累加")
print()

print("5. 为什么 16 个 beams 的 tokens 要分别统计？")
print("-" * 70)
print("   因为：")
print("   - 每个 beam 生成不同的 completion")
print("   - 这些 completions 都是独立的候选答案")
print("   - 计算总的 token 消耗时，需要把所有候选的 tokens 都计入")
print()
print("   例如验证结果中：")
print("   - 记录 3: 16 个 beams，completion_tokens: [273, 284, 320, 285, ...]")
print("   - total_active_beam_tokens: 4648 = 273+284+320+285+... (16个数字之和)")
print("   - 这表示这 16 个候选答案一共生成了 4648 个 tokens")
print()

print("6. 与 prompt tokens 的对比：")
print("-" * 70)
print("   如果要统计包括 prompt 在内的总 token 消耗：")
print("   - prompt_tokens: 假设 100 tokens")
print("   - 16 个 beams 共享同一个 prompt: 100 tokens (只算一次)")
print("   - completion_tokens: 4648 tokens (16 个 beams 的总和)")
print("   - 总消耗: 100 + 4648 = 4748 tokens")
print()
print("   但我们的 total_active_beam_tokens 只统计 completion 部分：4648")
print()

print("=" * 70)
print("结论")
print("=" * 70)
print()
print("✅ total_active_beam_tokens 不包含 prompt tokens")
print("✅ 不重复计算 prefix（已生成的部分）")
print("✅ 只统计每个 beam 新生成的 tokens 的累计")
print("✅ 16 个 beams 的 tokens 分别累加（因为它们是不同的候选）")
print()
print("这个统计方式是正确的，因为它反映了:")
print("- 生成这些候选答案实际消耗的计算资源")
print("- 每个 beam 都需要独立生成和评估")
print()
print("=" * 70)
