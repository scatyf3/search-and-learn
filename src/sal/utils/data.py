# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
import time
import json
from pathlib import Path
from datetime import datetime
import glob
import os

from datasets import Dataset, load_dataset
from huggingface_hub import (
    create_branch,
    list_repo_commits,
    repo_exists,
)

from sal.config import Config

logger = logging.getLogger()


def get_dataset(config: Config) -> Dataset:
    dataset = load_dataset(config.dataset_name, split=config.dataset_split)

    if config.dataset_start is not None and config.dataset_end is not None:
        dataset = dataset.select(range(config.dataset_start, config.dataset_end))
    if config.num_samples is not None:
        dataset = dataset.select(range(min(len(dataset), config.num_samples)))
    
    return dataset


def get_processed_indices(config: Config) -> set:
    """æ£€æŸ¥è¾“å‡ºç›®å½•ä¸­å·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œè¿”å›å·²å¤„ç†çš„æ ·æœ¬ç´¢å¼•é›†åˆ"""
    if config.output_dir is None:
        config.output_dir = f"data/{config.model_path}"
    
    if not os.path.exists(config.output_dir):
        return set()
    
    # æ„å»ºæ–‡ä»¶åæ¨¡å¼ï¼Œä¸ save_dataset ä¿æŒä¸€è‡´
    n_str = f"_n{config.n}" if hasattr(config, "n") and config.n is not None else ""
    
    # æ·»åŠ  temperature å’Œ strategy å‚æ•°ï¼ˆä¸ save_dataset é€»è¾‘ä¸€è‡´ï¼‰
    temp_str = ""
    strategy_str = ""
    if hasattr(config, "beam_decay_temperature") and config.beam_decay_temperature is not None:
        temp_str = f"_temp{config.beam_decay_temperature}"
    if hasattr(config, "beam_decay_strategy") and config.beam_decay_strategy is not None:
        strategy_str = f"_{config.beam_decay_strategy}"
    
    pattern = f"{config.output_dir}/{config.approach}{n_str}{temp_str}{strategy_str}_*_completions.jsonl"
    
    existing_files = glob.glob(pattern)
    if not existing_files:
        return set()
    
    # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(existing_files, key=os.path.getmtime)
    logger.info(f"ğŸ” å‘ç°å·²å­˜åœ¨æ–‡ä»¶: {latest_file}")
    
    processed_indices = set()
    try:
        with open(latest_file, 'r') as f:
            for idx, line in enumerate(f):
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                try:
                    data = json.loads(line)
                    # å‡è®¾æ•°æ®é›†æŒ‰é¡ºåºå¤„ç†ï¼Œä½¿ç”¨è¡Œå·ä½œä¸ºç´¢å¼•ï¼ˆå‡å»é…ç½®è¡Œï¼‰
                    processed_indices.add(len(processed_indices))
                except json.JSONDecodeError:
                    continue
        
        if processed_indices:
            logger.info(f"âœ… ä» {latest_file} åŠ è½½äº† {len(processed_indices)} ä¸ªå·²å¤„ç†æ ·æœ¬")
    except Exception as e:
        logger.warning(f"âš ï¸  è¯»å–å·²å­˜åœ¨æ–‡ä»¶å¤±è´¥: {e}")
        return set()
    
    return processed_indices


def save_dataset(dataset, config):
    if config.push_to_hub:
        # Since concurrent pushes can get rejected by the Hub, we make several attempts to push the dataset with try/except
        for _ in range(20):
            try:
                # Create branch from the repo's initial commit.
                # This is needed to avoid branching from a commit on main that already has data
                if repo_exists(config.hub_dataset_id, repo_type="dataset"):
                    initial_commit = list_repo_commits(
                        config.hub_dataset_id, repo_type="dataset"
                    )[-1]
                    create_branch(
                        repo_id=config.hub_dataset_id,
                        branch=config.revision,
                        revision=initial_commit.commit_id,
                        exist_ok=True,
                        repo_type="dataset",
                    )
                url = dataset.push_to_hub(
                    config.hub_dataset_id,
                    revision=config.revision,
                    split="train",
                    private=config.hub_dataset_private,
                    commit_message=f"Add {config.revision}",
                )
                break
            except Exception as e:
                logger.error(f"Error pushing dataset to the Hub: {e}")
                time.sleep(5)
        logger.info(f"Pushed dataset to {url}")
    else:
        if config.output_dir is None:
            config.output_dir = f"data/{config.model_path}"
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)
        
        # æ–‡ä»¶ååŠ å…¥nå‚æ•°å’Œå…¶ä»–å‚æ•°
        n_str = f"_n{config.n}" if hasattr(config, "n") and config.n is not None else ""
        
        # æ·»åŠ  temperature å’Œ strategy å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        temp_str = ""
        strategy_str = ""
        if hasattr(config, "beam_decay_temperature") and config.beam_decay_temperature is not None:
            temp_str = f"_temp{config.beam_decay_temperature}"
        if hasattr(config, "beam_decay_strategy") and config.beam_decay_strategy is not None:
            strategy_str = f"_{config.beam_decay_strategy}"
        
        params_str = f"{n_str}{temp_str}{strategy_str}"
        
        # æ€»æ˜¯ç”Ÿæˆæ–°æ–‡ä»¶ï¼šç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        out_path = f"{config.output_dir}/{config.approach}{params_str}_{timestamp}_completions.jsonl"
        
        # ä¿å­˜é…ç½®å¤´
        config_dict = config.__dict__.copy()
        config_dict["timestamp"] = timestamp
        
        with open(out_path, 'w') as f:
            f.write(f"# CONFIG: {json.dumps(config_dict, ensure_ascii=False)}\n")
        
        logger.info(f"âœ¨ åˆ›å»ºæ–°æ–‡ä»¶: {out_path}")

        # ä¿å­˜æ•°æ®é›†å†…å®¹ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        dataset.to_json(out_path, lines=True, mode='a')
        logger.info(f"ğŸ’¾ å·²ä¿å­˜ {len(dataset)} æ¡è®°å½•åˆ° {out_path}")
