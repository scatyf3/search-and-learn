import time
import torch
import numpy as np
from sal.config import Config
from sal.models.reward_models import PRM
from sal.utils.score import aggregate_scores

def best_of_n_speculative(x, config: Config, llm, tokenizer, draft_model, draft_tokenizer, prm: PRM):
    """
    Speculative decoding with Best-of-N.
    Forces num_return_sequences=1 inside a loop to support assisted_generation.
    """
    print("Running best_of_n_speculative...")
    
    # 1. å‡†å¤‡ Prompts
    prompts = []
    for prompt in x["problem"]:
        full_prompt = config.system_prompt + "\n" + prompt
        prompts.append(full_prompt)

    # åˆå§‹åŒ–å®¹å™¨ [Batch_Size, N]
    completions = [[] for _ in range(len(prompts))]
    completion_tokens = [[] for _ in range(len(prompts))]

    # ä¿®æ­£ç‚¹ 1: åˆå§‹åŒ–è€—æ—¶åˆ—è¡¨
    llm_gen_times = []

    # 2. éå† Batch (ç”Ÿæˆé˜¶æ®µ)
    for i, prompt in enumerate(prompts):
        
        # ä¿®æ­£ç‚¹ 2: å¼€å§‹è®¡æ—¶ (é’ˆå¯¹å½“å‰è¿™ä¸ªé—®é¢˜)
        t_problem_start = time.time()
        
        inputs = tokenizer(prompt, return_tensors="pt").to(llm.device)
        
        prompt_candidates = []
        prompt_lens = []

        # æ ¸å¿ƒå¾ªç¯ï¼šç”Ÿæˆ N ä¸ªå€™é€‰é¡¹
        for _ in range(config.n):
            outputs = llm.generate(
                **inputs,
                assistant_model=draft_model,  # å¯ç”¨ Speculative Decoding
                max_new_tokens=config.max_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                num_return_sequences=1,       # å¿…é¡»ä¸º 1
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            
            # è§£ç å¹¶å»é™¤ prompt
            input_len = inputs.input_ids.shape[1]
            generated_ids = outputs[0][input_len:] # åˆ‡æ‰ prompt
            text_only = tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            prompt_candidates.append(text_only)
            prompt_lens.append(len(generated_ids))

        completions[i] = prompt_candidates
        completion_tokens[i] = prompt_lens
        
        # ä¿®æ­£ç‚¹ 3: ç»“æŸè®¡æ—¶å¹¶è®°å½•
        t_problem_end = time.time()
        llm_gen_times.append(t_problem_end - t_problem_start)

    # å®Œæ•´æ€§æ£€æŸ¥
    for c in completions:
        if len(c) != config.n:
            raise ValueError(f"Generated {len(c)} completions instead of {config.n}")

    # 3. PRM è¯„åˆ†
    t_prm_start = time.time()
    scores = prm.score(x["problem"], completions, batch_size=config.prm_batch_size)
    t_prm_end = time.time()
    prm_score_time = t_prm_end - t_prm_start


    # 4. èšåˆåˆ†æ•°ä¸é¢„æµ‹
    agg_scores = [
        [aggregate_scores(s, config.agg_strategy) for s in score] for score in scores
    ]
    

    # é€‰å‡ºæœ€ä½³
    pred = [completion[np.argmax(s)] for completion, s in zip(completions, agg_scores)]
    
    # ç®—å¹³å‡æ¯ä¸ªé—®é¢˜çš„ PRM è€—æ—¶ (PRMé€šå¸¸æ˜¯Batchçš„ï¼Œæ‰€ä»¥è¿™é‡Œç®—å¹³å‡æ¯”è¾ƒåˆç†)
    avg_prm_time = prm_score_time / len(x["problem"]) if len(x["problem"]) > 0 else 0

    # 5. è¿”å›ç»“æœ
    '''
    x["completions"] = completions
    x["scores"] = scores
    x["pred"] = pred
    x["completion_tokens"] = completion_tokens
    ä½†æ˜¯ä¹‹å‰çš„å­—æ®µæœ‰problem, solution, levelç­‰ç­‰ï¼Œä¸ºå•¥è¿™é‡ŒğŸˆšï¸äº†
    '''
    x["completions"] = completions
    x["scores"] = scores
    x["pred"] = pred
    x["completion_tokens"] = completion_tokens
    
    x["llm_gen_time"] = llm_gen_times 
    x["prm_score_time"] = [avg_prm_time] * len(x["problem"])
    
    return x