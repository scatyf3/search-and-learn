#!/usr/bin/env python
"""å¯¹æ¯”å‰50ä¸ªé—®é¢˜çš„æ—¶é—´å’Œtokenç»Ÿè®¡"""

import json

baseline_file = "data/meta-llama/Llama-3.2-1B-Instruct/beam_search_n16_temp1.0_exp_20251212_084748_completions.jsonl"
dynamic_file = "data/meta-llama/Llama-3.2-1B-Instruct/beam_search_dynamic_n16_temp0.8_exp_20251213_192225_completions.jsonl"

print("=" * 80)
print("Baseline vs Dynamic Beam Search åŠ é€Ÿæ¯”åˆ†æï¼ˆå‰50ä¸ªé—®é¢˜ï¼‰")
print("=" * 80)
print()

# è¯»å–ä¸¤ä¸ªæ–‡ä»¶çš„å‰50æ¡
baseline_data = []
dynamic_data = []

with open(baseline_file, 'r') as f:
    for i, line in enumerate(f):
        if i >= 50:
            break
        if line.strip() and not line.startswith('#'):
            baseline_data.append(json.loads(line))

with open(dynamic_file, 'r') as f:
    for i, line in enumerate(f):
        if i >= 50:
            break
        if line.strip() and not line.startswith('#'):
            dynamic_data.append(json.loads(line))

print(f"è¯»å–æ•°æ®: Baseline {len(baseline_data)} æ¡, Dynamic {len(dynamic_data)} æ¡")
print()

# ç»Ÿè®¡
baseline_llm_time = sum(d.get('llm_gen_time', 0) for d in baseline_data)
baseline_prm_time = sum(d.get('prm_score_time', 0) for d in baseline_data)
baseline_total_time = baseline_llm_time + baseline_prm_time
baseline_tokens = sum(sum(d.get('completion_tokens', [])) for d in baseline_data)

dynamic_llm_time = sum(d.get('llm_gen_time', 0) for d in dynamic_data)
dynamic_prm_time = sum(d.get('prm_score_time', 0) for d in dynamic_data)
dynamic_total_time = sum(d.get('total_time_beam_search', 0) for d in dynamic_data)
dynamic_tokens = sum(d.get('total_generated_tokens', 0) for d in dynamic_data)
dynamic_active_tokens = sum(d.get('total_active_beam_tokens', 0) for d in dynamic_data)
dynamic_pruned_tokens = sum(d.get('total_pruned_tokens', 0) for d in dynamic_data)

print("=" * 80)
print("1. Token ç»Ÿè®¡")
print("=" * 80)
print(f"\n{'æŒ‡æ ‡':<40} {'Baseline':<15} {'Dynamic':<15} {'æ¯”ç‡':<15}")
print("-" * 80)
print(f"{'æ€»ç”Ÿæˆ tokens':<40} {baseline_tokens:>14,} {dynamic_tokens:>14,} {dynamic_tokens/baseline_tokens:>14.2%}")
print(f"{'å¹³å‡ tokens/é—®é¢˜':<40} {baseline_tokens/50:>14.1f} {dynamic_tokens/50:>14.1f} {(dynamic_tokens/50)/(baseline_tokens/50):>14.2%}")

if dynamic_active_tokens > 0:
    print(f"{'  - Active beam tokens':<40} {'N/A':>14} {dynamic_active_tokens:>14,} {'':>15}")
    print(f"{'  - Pruned tokens':<40} {'N/A':>14} {dynamic_pruned_tokens:>14,} {'':>15}")
    prune_rate = dynamic_pruned_tokens / dynamic_tokens * 100
    print(f"{'  - Prune rate':<40} {'N/A':>14} {prune_rate:>13.1f}% {'':>15}")

token_reduction = (1 - dynamic_tokens / baseline_tokens) * 100
token_speedup = baseline_tokens / dynamic_tokens

print()
print(f"ğŸ’¡ Token å‡å°‘: {token_reduction:.1f}%")
print(f"ğŸ’¡ Token åŠ é€Ÿæ¯”: {token_speedup:.2f}x")
print()

print("=" * 80)
print("2. æ—¶é—´ç»Ÿè®¡")
print("=" * 80)
print(f"\n{'æŒ‡æ ‡':<40} {'Baseline':<15} {'Dynamic':<15} {'æ¯”ç‡':<15}")
print("-" * 80)
print(f"{'LLM ç”Ÿæˆæ—¶é—´ (ç§’)':<40} {baseline_llm_time:>14.1f} {dynamic_llm_time:>14.1f} {dynamic_llm_time/baseline_llm_time:>14.2%}")
print(f"{'PRM è¯„åˆ†æ—¶é—´ (ç§’)':<40} {baseline_prm_time:>14.1f} {dynamic_prm_time:>14.1f} {dynamic_prm_time/baseline_prm_time:>14.2%}")
print(f"{'æ€»æ—¶é—´ (ç§’)':<40} {baseline_total_time:>14.1f} {dynamic_total_time:>14.1f} {dynamic_total_time/baseline_total_time:>14.2%}")
print()
print(f"{'å¹³å‡ LLM æ—¶é—´/é—®é¢˜ (ç§’)':<40} {baseline_llm_time/50:>14.2f} {dynamic_llm_time/50:>14.2f} {(dynamic_llm_time/50)/(baseline_llm_time/50):>14.2%}")
print(f"{'å¹³å‡ PRM æ—¶é—´/é—®é¢˜ (ç§’)':<40} {baseline_prm_time/50:>14.2f} {dynamic_prm_time/50:>14.2f} {(dynamic_prm_time/50)/(baseline_prm_time/50):>14.2%}")
print(f"{'å¹³å‡æ€»æ—¶é—´/é—®é¢˜ (ç§’)':<40} {baseline_total_time/50:>14.2f} {dynamic_total_time/50:>14.2f} {(dynamic_total_time/50)/(baseline_total_time/50):>14.2%}")

time_reduction = (1 - dynamic_total_time / baseline_total_time) * 100
time_speedup = baseline_total_time / dynamic_total_time

print()
print(f"ğŸ’¡ æ€»æ—¶é—´èŠ‚çœ: {time_reduction:.1f}%")
print(f"ğŸ’¡ æ€»æ—¶é—´åŠ é€Ÿæ¯”: {time_speedup:.2f}x")
print()

# åˆ†è§£æ—¶é—´åŠ é€Ÿæ¯”
llm_speedup = baseline_llm_time / dynamic_llm_time if dynamic_llm_time > 0 else 0
prm_speedup = baseline_prm_time / dynamic_prm_time if dynamic_prm_time > 0 else 0

print(f"  - LLM ç”ŸæˆåŠ é€Ÿæ¯”: {llm_speedup:.2f}x")
print(f"  - PRM è¯„åˆ†åŠ é€Ÿæ¯”: {prm_speedup:.2f}x")
print()

print("=" * 80)
print("3. æ•ˆç‡åˆ†æ")
print("=" * 80)
print()

# Tokens per second
baseline_tps = baseline_tokens / baseline_llm_time if baseline_llm_time > 0 else 0
dynamic_tps = dynamic_tokens / dynamic_llm_time if dynamic_llm_time > 0 else 0

print(f"LLM ç”Ÿæˆé€Ÿåº¦:")
print(f"  Baseline: {baseline_tps:>8.1f} tokens/sec")
print(f"  Dynamic:  {dynamic_tps:>8.1f} tokens/sec")
print()

# Time per token
baseline_ms_per_token = baseline_llm_time * 1000 / baseline_tokens if baseline_tokens > 0 else 0
dynamic_ms_per_token = dynamic_llm_time * 1000 / dynamic_tokens if dynamic_tokens > 0 else 0

print(f"æ¯ä¸ª token çš„ LLM ç”Ÿæˆæ—¶é—´:")
print(f"  Baseline: {baseline_ms_per_token:.3f} ms/token")
print(f"  Dynamic:  {dynamic_ms_per_token:.3f} ms/token")
print()

# Time breakdown
print(f"æ—¶é—´å æ¯”:")
print(f"  Baseline: LLM {baseline_llm_time/baseline_total_time*100:.1f}%, PRM {baseline_prm_time/baseline_total_time*100:.1f}%")
print(f"  Dynamic:  LLM {dynamic_llm_time/dynamic_total_time*100:.1f}%, PRM {dynamic_prm_time/dynamic_total_time*100:.1f}%")
print()

print("=" * 80)
print("4. å…³é”®å‘ç°")
print("=" * 80)
print()

print(f"ğŸš€ æ•´ä½“åŠ é€Ÿæ¯”: {time_speedup:.2f}x ({time_reduction:.1f}% æ—¶é—´èŠ‚çœ)")
print()

print(f"ğŸ“Š åˆ†è§£åˆ†æ:")
print(f"  â€¢ Token å‡å°‘: {token_speedup:.2f}x ({token_reduction:.1f}% èŠ‚çœ)")
print(f"  â€¢ LLM ç”ŸæˆåŠ é€Ÿ: {llm_speedup:.2f}x")
print(f"  â€¢ PRM è¯„åˆ†åŠ é€Ÿ: {prm_speedup:.2f}x")
print()

# åˆ†æä¸åŒä¹‹å¤„
if llm_speedup > token_speedup:
    print(f"âœ… LLM åŠ é€Ÿæ¯” ({llm_speedup:.2f}x) > Token å‡å°‘æ¯” ({token_speedup:.2f}x)")
    print(f"   è¯´æ˜: Dynamic é™¤äº†å‡å°‘ tokensï¼Œè¿˜æå‡äº† LLM ç”Ÿæˆæ•ˆç‡")
elif token_speedup > llm_speedup:
    diff = token_speedup - llm_speedup
    print(f"âš ï¸  Token å‡å°‘æ¯” ({token_speedup:.2f}x) > LLM åŠ é€Ÿæ¯” ({llm_speedup:.2f}x) [å·®å¼‚: {diff:.2f}x]")
    print(f"   è¯´æ˜: è™½ç„¶å‡å°‘äº† {token_reduction:.1f}% çš„ tokensï¼Œä½† LLM åŠ é€Ÿæ¯”æ›´å°")
    print(f"   å¯èƒ½åŸå› : Dynamic beam search çš„é¢å¤–å¼€é”€ï¼ˆbeamç®¡ç†ã€pruningç­‰ï¼‰")
else:
    print(f"âœ… LLM åŠ é€Ÿæ¯” â‰ˆ Token å‡å°‘æ¯” ({llm_speedup:.2f}x)")
    print(f"   è¯´æ˜: Token å‡å°‘ç›´æ¥è½¬åŒ–ä¸º LLM æ—¶é—´èŠ‚çœ")

print()

if prm_speedup < 1.0:
    print(f"âš ï¸  PRM è¯„åˆ†æ—¶é—´å¢åŠ : {1/prm_speedup:.2f}x")
    print(f"   Dynamic: {dynamic_prm_time:.1f}ç§’ vs Baseline: {baseline_prm_time:.1f}ç§’")
    print(f"   å¯èƒ½åŸå› : æ›´é¢‘ç¹çš„è¯„åˆ†ã€æ›´å¤šçš„ä¸­é—´æ­¥éª¤è¯„ä¼°")
elif prm_speedup > 1.0:
    print(f"âœ… PRM è¯„åˆ†åŠ é€Ÿ: {prm_speedup:.2f}x")
    print(f"   è¯´æ˜: Pruning å‡å°‘äº†éœ€è¦è¯„åˆ†çš„ beams")

print()
print("=" * 80)
print("5. æ€»ç»“")
print("=" * 80)
print()

print(f"å¯¹æ¯”ç›¸åŒçš„ 50 ä¸ªé—®é¢˜:")
print(f"  â€¢ Baseline: å›ºå®š 16 beamsï¼Œæ—  pruning")
print(f"  â€¢ Dynamic: åŠ¨æ€ beam widthï¼Œ25% prune rate")
print()
print(f"ç»“æœ:")
print(f"  âœ… æ€»æ—¶é—´åŠ é€Ÿ: {time_speedup:.2f}x (èŠ‚çœ {time_reduction:.1f}% æ—¶é—´)")
print(f"  âœ… Token å‡å°‘: {token_speedup:.2f}x (èŠ‚çœ {token_reduction:.1f}% tokens)")
print(f"  âœ… LLM ç”ŸæˆåŠ é€Ÿ: {llm_speedup:.2f}x")
print(f"  {'âœ…' if prm_speedup > 1.0 else 'âš ï¸ '} PRM è¯„åˆ†å˜åŒ–: {prm_speedup:.2f}x")
print()

if time_speedup < 1.5:
    print(f"ğŸ’¡ æ”¹è¿›å»ºè®®:")
    print(f"  - å½“å‰åŠ é€Ÿæ¯” {time_speedup:.2f}x ç›¸å¯¹æœ‰é™")
    print(f"  - å»ºè®®: æ›´æ¿€è¿›çš„ early pruning (å½“å‰ prune rate: {prune_rate:.1f}%)")
    print(f"  - å»ºè®®: æ›´å¿«çš„ beam width è¡°å‡")
    if prm_speedup < 1.0:
        print(f"  - å»ºè®®: ä¼˜åŒ– PRM è¯„åˆ†é¢‘ç‡ï¼ˆå½“å‰ PRM æ—¶é—´å¢åŠ äº† {(1/prm_speedup - 1)*100:.1f}%ï¼‰")

print()
print("=" * 80)
