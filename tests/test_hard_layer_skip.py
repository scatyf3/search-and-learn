import time
import torch
import copy
from transformers import AutoModelForCausalLM, AutoTokenizer

# 1. è®¾ç½®æ¨¡å‹å’Œç¯å¢ƒ
checkpoint = "facebook/layerskip-llama3.2-1B"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device.upper()}")

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenizer.pad_token = tokenizer.eos_token # Batching å¿…é¡»è®¾ç½®

# 2. åŠ è½½å®Œæ•´æ¨¡å‹
print(">>> åŠ è½½å®Œæ•´æ¨¡å‹...")
model_full = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)

# 3. åˆ¶é€ ç‰©ç†æˆªæ–­æ¨¡å‹ (Layer Skip)
print(">>> åˆ¶é€ æˆªæ–­ç‰ˆæ¨¡å‹ (åªä¿ç•™å‰4å±‚)...")
model_skip = copy.deepcopy(model_full)
EXIT_LAYER = 4
# ç‰©ç†åˆ é™¤ç¬¬4å±‚ä¹‹åçš„æ‰€æœ‰å±‚ï¼Œè¿™æ˜¯å®ç° Batching åŠ é€Ÿçš„å”¯ä¸€åŠæ³•
model_skip.model.layers = model_skip.model.layers[:EXIT_LAYER]
model_skip.config.num_hidden_layers = EXIT_LAYER

# 4. å‡†å¤‡ Batch æ•°æ® (8æ¡ä¸åŒçš„æç¤ºè¯)
prompts = [
    "Alice and Bob are playing",
    "The capital of France is",
    "Python is a programming language",
    "The quick brown fox jumps",
    "Machine learning is fascinating",
    "To be or not to be",
    "I like to eat pizza",
    "Winter is coming"
]
inputs = tokenizer(prompts, return_tensors="pt", padding=True).to(device)

gen_kwargs = {
    "do_sample": False,
    "max_new_tokens": 30, # ç¨å¾®çŸ­ä¸€ç‚¹ï¼Œæ–¹ä¾¿é˜…è¯»
    "pad_token_id": tokenizer.eos_token_id
}

print("-" * 60)
print(f"Batch Size: {len(prompts)} | Layer Skip: ä¿ç•™å‰ {EXIT_LAYER} å±‚")
print("-" * 60)

# --- è¿è¡Œå®Œæ•´æ¨¡å‹ ---
print(">>> æ­£åœ¨ç”Ÿæˆ: Full Model (16 Layers)...")
torch.cuda.synchronize()
start = time.perf_counter()
outputs_full = model_full.generate(**inputs, **gen_kwargs)
torch.cuda.synchronize()
time_full = time.perf_counter() - start

# --- è¿è¡Œæˆªæ–­æ¨¡å‹ ---
print(f">>> æ­£åœ¨ç”Ÿæˆ: Truncated Model ({EXIT_LAYER} Layers)...")
torch.cuda.synchronize()
start = time.perf_counter()
outputs_skip = model_skip.generate(**inputs, **gen_kwargs)
torch.cuda.synchronize()
time_skip = time.perf_counter() - start

# --- è§£ç æ–‡æœ¬ ---
text_full = tokenizer.batch_decode(outputs_full, skip_special_tokens=True)
text_skip = tokenizer.batch_decode(outputs_skip, skip_special_tokens=True)

# --- æ‰“å°è¯¦ç»†å¯¹æ¯”ç»“æœ ---
print("\n" + "=" * 80)
print(" " * 30 + "ç”Ÿæˆå†…å®¹å¯¹æ¯”")
print("=" * 80)

for i in range(len(prompts)):
    print(f"\n[Sample {i+1}]: Input = '{prompts[i]}'")
    print("-" * 40)
    print(f"ğŸ”´ Full (16å±‚): {text_full[i].replace(prompts[i], '...').strip()}")
    print(f"ğŸŸ¢ Skip ( 4å±‚): {text_skip[i].replace(prompts[i], '...').strip()}")

print("\n" + "=" * 80)
print(f"Full Batch è€—æ—¶: {time_full:.4f}s")
print(f"Skip Batch è€—æ—¶: {time_skip:.4f}s")
speedup = time_full / time_skip
print(f"ğŸš€ çœŸå®åŠ é€Ÿå€ç‡: {speedup:.2f}x")
print("=" * 80)