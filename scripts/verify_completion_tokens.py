#!/usr/bin/env python
"""
éªŒè¯ completion_tokens å­—æ®µçš„å‡†ç¡®æ€§

è¯¥è„šæœ¬ä¼šï¼š
1. è¯»å– JSONL æ–‡ä»¶ä¸­çš„ completions å’Œ completion_tokens
2. ä½¿ç”¨ tokenizer é‡æ–°è®¡ç®—æ¯ä¸ª completion çš„ token æ•°é‡
3. å¯¹æ¯”å®é™…å€¼å’Œè®°å½•å€¼ï¼ŒæŠ¥å‘Šå·®å¼‚
"""

import json
import sys
from pathlib import Path
from transformers import AutoTokenizer
from collections import defaultdict

'''
data/meta-llama/Llama-3.2-1B-Instruct/beam_search_n4_temp1.0_exp_20251210_141452_completions.jsonl
data/meta-llama/Llama-3.2-1B-Instruct/beam_search_n4_temp1.0_exp_20251210_143633_completions.jsonl

'''

def verify_tokens(jsonl_path: str, model_path: str = "meta-llama/Llama-3.2-1B-Instruct"):
    """éªŒè¯ JSONL æ–‡ä»¶ä¸­çš„ completion_tokens æ˜¯å¦å‡†ç¡®"""
    
    print(f"ğŸ“ åŠ è½½æ–‡ä»¶: {jsonl_path}")
    print(f"ğŸ¤– åŠ è½½ tokenizer: {model_path}")
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_examples': 0,
        'total_completions': 0,
        'exact_matches': 0,
        'mismatches': 0,
        'errors': 0,
        'max_diff': 0,
        'diffs': []
    }
    
    # è¯»å– JSONL æ–‡ä»¶
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            try:
                data = json.loads(line)
                stats['total_examples'] += 1
                
                # è·å– completions å’Œ completion_tokens
                completions = data.get('completions', [])
                recorded_tokens = data.get('completion_tokens', [])
                
                if len(completions) != len(recorded_tokens):
                    print(f"âš ï¸  è¡Œ {line_num}: completions æ•°é‡ ({len(completions)}) ä¸ completion_tokens æ•°é‡ ({len(recorded_tokens)}) ä¸åŒ¹é…")
                    stats['errors'] += 1
                    continue
                
                # é€ä¸ªéªŒè¯
                for idx, (completion, recorded) in enumerate(zip(completions, recorded_tokens)):
                    stats['total_completions'] += 1
                    
                    # ä½¿ç”¨ tokenizer è®¡ç®—å®é™… token æ•°
                    actual = len(tokenizer.encode(completion, add_special_tokens=False))
                    
                    diff = abs(actual - recorded)
                    
                    if actual == recorded:
                        stats['exact_matches'] += 1
                    else:
                        stats['mismatches'] += 1
                        stats['diffs'].append(diff)
                        stats['max_diff'] = max(stats['max_diff'], diff)
                        
                        # æ‰“å°å‰å‡ ä¸ªä¸åŒ¹é…çš„ç¤ºä¾‹
                        if stats['mismatches'] <= 5:
                            print(f"\nâŒ ä¸åŒ¹é… (è¡Œ {line_num}, completion {idx}):")
                            print(f"   è®°å½•å€¼: {recorded}")
                            print(f"   å®é™…å€¼: {actual}")
                            print(f"   å·®å¼‚: {diff}")
                            print(f"   æ–‡æœ¬é•¿åº¦: {len(completion)} å­—ç¬¦")
                            if len(completion) < 100:
                                print(f"   æ–‡æœ¬: {completion[:100]}...")
            
            except json.JSONDecodeError as e:
                print(f"âš ï¸  è¡Œ {line_num}: JSON è§£æé”™è¯¯ - {e}")
                stats['errors'] += 1
            except Exception as e:
                print(f"âš ï¸  è¡Œ {line_num}: å¤„ç†é”™è¯¯ - {e}")
                stats['errors'] += 1
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š éªŒè¯ç»“æœç»Ÿè®¡")
    print("="*60)
    print(f"æ€»æ ·æœ¬æ•°: {stats['total_examples']}")
    print(f"æ€» completion æ•°: {stats['total_completions']}")
    print(f"âœ… å®Œå…¨åŒ¹é…: {stats['exact_matches']} ({stats['exact_matches']/max(stats['total_completions'],1)*100:.1f}%)")
    print(f"âŒ ä¸åŒ¹é…: {stats['mismatches']} ({stats['mismatches']/max(stats['total_completions'],1)*100:.1f}%)")
    print(f"âš ï¸  é”™è¯¯: {stats['errors']}")
    
    if stats['mismatches'] > 0:
        print(f"\nå·®å¼‚ç»Ÿè®¡:")
        print(f"  æœ€å¤§å·®å¼‚: {stats['max_diff']} tokens")
        print(f"  å¹³å‡å·®å¼‚: {sum(stats['diffs'])/len(stats['diffs']):.2f} tokens")
        print(f"  ä¸­ä½æ•°å·®å¼‚: {sorted(stats['diffs'])[len(stats['diffs'])//2]} tokens")
    
    print("="*60)
    
    # è¿”å›æ˜¯å¦å…¨éƒ¨å‡†ç¡®
    return stats['mismatches'] == 0 and stats['errors'] == 0


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python verify_completion_tokens.py <jsonl_file> [model_path]")
        print("\nç¤ºä¾‹:")
        print("  python verify_completion_tokens.py data/meta-llama/Llama-3.2-1B-Instruct/beam_search_n4_*.jsonl")
        print("  python verify_completion_tokens.py data/results.jsonl meta-llama/Llama-3.2-3B-Instruct")
        sys.exit(1)
    
    jsonl_path = sys.argv[1]
    model_path = sys.argv[2] if len(sys.argv) > 2 else "meta-llama/Llama-3.2-1B-Instruct"
    
    if not Path(jsonl_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}")
        sys.exit(1)
    
    is_accurate = verify_tokens(jsonl_path, model_path)
    
    if is_accurate:
        print("\nâœ… æ‰€æœ‰ completion_tokens éƒ½å‡†ç¡®ï¼")
        sys.exit(0)
    else:
        print("\nâš ï¸  å‘ç°ä¸å‡†ç¡®çš„ completion_tokens")
        sys.exit(1)


if __name__ == "__main__":
    main()

'''
(base) [yf3005@ga007 search-and-learn]$ python scripts/calculate_tokens_per_second.py data/meta-llama/Llama-3.2-1B-Instruct/beam_search_n4_temp1.0_exp_20251210_143633_completions.jsonl
Line 1: LLM Gen - 167.22 tokens/s
Line 1: PRM Score - 437.62 tokens/s
Line 2: LLM Gen - 56.49 tokens/s
Line 2: PRM Score - 76.03 tokens/s
Line 3: LLM Gen - 199.34 tokens/s
Line 3: PRM Score - 280.00 tokens/s
Line 4: LLM Gen - 172.63 tokens/s
Line 4: PRM Score - 509.62 tokens/s


(base) [yf3005@ga007 search-and-learn]$ python scripts/calculate_tokens_per_second.py data/meta-llama/Llama-3.2-1B-Instruct/beam_search_n4_temp1.0_exp_20251210_141452_completions.jsonl
Line 1: LLM Gen - 167.22 tokens/s
Line 1: PRM Score - 437.62 tokens/s
Line 2: LLM Gen - 56.49 tokens/s
Line 2: PRM Score - 76.03 tokens/s
Line 3: LLM Gen - 199.34 tokens/s
Line 3: PRM Score - 280.00 tokens/s
Line 4: LLM Gen - 172.63 tokens/s
Line 4: PRM Score - 509.62 tokens/s
'''