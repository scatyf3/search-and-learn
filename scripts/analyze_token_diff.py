#!/usr/bin/env python
"""
è¯¦ç»†åˆ†æ completion_tokens çš„è®¡ç®—å·®å¼‚

æ£€æŸ¥ï¼š
1. token_ids æ˜¯å¦åŒ…å« stop token
2. æ–‡æœ¬ç¼–ç å’Œ token_ids çš„å·®å¼‚
3. special tokens çš„å½±å“
"""

import json
from transformers import AutoTokenizer

def analyze_first_completion():
    jsonl_path = "data/meta-llama/Llama-3.2-1B-Instruct/beam_search_dynamic_n4_temp0.8_exp_20251208_225253_completions.jsonl"
    model_path = "meta-llama/Llama-3.2-1B-Instruct"
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    with open(jsonl_path, 'r') as f:
        data = json.loads(f.readline())
    
    completion = data['completions'][0]
    recorded_tokens = data['completion_tokens'][0]
    
    print("="*70)
    print("ğŸ“ ç¬¬ä¸€ä¸ª completion åˆ†æ")
    print("="*70)
    print(f"\næ–‡æœ¬é•¿åº¦: {len(completion)} å­—ç¬¦")
    print(f"è®°å½•çš„ token æ•°: {recorded_tokens}")
    
    # æ–¹æ³•1: ä¸åŠ  special tokens
    tokens_no_special = tokenizer.encode(completion, add_special_tokens=False)
    print(f"\næ–¹æ³•1 - encode(add_special_tokens=False): {len(tokens_no_special)}")
    
    # æ–¹æ³•2: åŠ  special tokens  
    tokens_with_special = tokenizer.encode(completion, add_special_tokens=True)
    print(f"æ–¹æ³•2 - encode(add_special_tokens=True): {len(tokens_with_special)}")
    
    # æ–¹æ³•3: tokenize
    tokens = tokenizer.tokenize(completion)
    print(f"æ–¹æ³•3 - tokenize(): {len(tokens)}")
    
    # æŸ¥çœ‹å‰å‡ ä¸ªå’Œåå‡ ä¸ª token
    print(f"\nå‰ 5 ä¸ª tokens (no special):")
    for i, tid in enumerate(tokens_no_special[:5]):
        print(f"  [{i}] {tid}: '{tokenizer.decode([tid])}'")
    
    print(f"\nå 5 ä¸ª tokens (no special):")
    for i, tid in enumerate(tokens_no_special[-5:], len(tokens_no_special)-5):
        print(f"  [{i}] {tid}: '{tokenizer.decode([tid])}'")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®Šå­—ç¬¦
    if completion.endswith('\n\n'):
        print(f"\nâš ï¸  æ–‡æœ¬ä»¥ \\n\\n ç»“å°¾")
        without_stop = completion[:-2]
        tokens_without_stop = tokenizer.encode(without_stop, add_special_tokens=False)
        print(f"   å»é™¤ \\n\\n åçš„ token æ•°: {len(tokens_without_stop)}")
        
        # å•ç‹¬ç¼–ç  \n\n
        stop_tokens = tokenizer.encode('\n\n', add_special_tokens=False)
        print(f"   \\n\\n çš„ token æ•°: {len(stop_tokens)}")
        print(f"   \\n\\n çš„ token IDs: {stop_tokens}")
        
    # å·®å¼‚åˆ†æ
    diff = recorded_tokens - len(tokens_no_special)
    print(f"\nğŸ“Š å·®å¼‚: {diff} tokens")
    
    if diff == 1:
        print("   å¯èƒ½åŸå› : vLLM çš„ token_ids å¯èƒ½åŒ…å«äº†ä¸€ä¸ªé¢å¤–çš„ token")
        print("   - å¯èƒ½æ˜¯ BOS/EOS token")
        print("   - å¯èƒ½æ˜¯ stop token çš„è®¡æ•°æ–¹å¼ä¸åŒ")
    
    # æ£€æŸ¥ç‰¹æ®Š tokens
    print(f"\nğŸ” ç‰¹æ®Š tokens:")
    print(f"   BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
    print(f"   EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    print(f"   PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    
    print("\n" + "="*70)

if __name__ == "__main__":
    analyze_first_completion()
